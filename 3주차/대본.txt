발표 시작하겠습니다.
지난주에 이어 Hands-On Machine Learning 2장 후반부 내용을 발표하겠습니다.

먼저 데이터 전처리 입니다.
데이터 전처리란, 모델 학습을 효율적으로 진행하기 위해 주어진 데이터를 변환시키는 것으로
( ) 수치형 데이터에 대해 NULL값 등을 처리하는 데이터 정제,
상관관계가 적은 특성들을 조합하여 관계를 높이는 조합 특성 추가,
각 특성의 크기를 맞춰주는 특성 스케일링 등이 있고,
범주형 데이터들의 단어 집합들 중 표현 하고 싶은 인덱스에 1, 다른 인덱스에는 0을 두는 원-핫-인코딩 등이 있습니다.

() 먼저 info() 메서드로 데이터를 살펴보면,
총 16512개의 데이터중 4번 feature total_bedrooms의 경우 16354개의 non-null값, 즉 158개의 NULL값을 가지고 있습니다.

대부분의 알고리즘은 누락된 특성을 다루지 못하므로,

( ) 해당 NULL값이 있는 데이터셋을 통체로 제거하거나,
total_bedrooms 특성 전체를 삭제하거나,
0이나 평균,중간값등의 특정값으로 채우는 방법등으로 데이터를 정제해야 합니다.

일반적으로는 구역을 제거하거나 특성을 삭제하는거 보단, 손실이 없는
특정값을 채우는 방식을 더 선호하며 이 경우, 그 특성의 성질을 최대한 잘 대표하는 값을 찾아내는것이 중요합니다.

( ) 중간값을 이용해 NULL값을 처리해 보자면,
먼저 sklearn의 SimpleImputer를 import하고, median, 중간값 방식으로 선언한 뒤, 
비 수치형 특성인 ocean_proximity를 제거한, 수치형 특성들만을 이용해 학습합니다.

( ) 그러면, imputer의 결과는 각 특성들의 중간값이 되며,
( ) 이를 imputer로 대치시키고,
결과로 나온 Numpy 배열을 Pandas의 DataFrame으로 변환시켜 보면,
해당 비어있던 NULL값들이 중간값으로 채워졌고,

( ) info() 메서드로 확인해 봤을때,
비어있던 total_bedrooms 특성의 non-null count가 16512개 모두로 나옴을 볼 수 있습니다.

( ) 그다음 describe() 메서드로 데이터들을 보면,
total_rooms의 경우 최소값이 6, 최대값이 39320 표준편차가 2138이며,
median_income의 경우 최소값이 0.5, 최대값이 15, 표준편차가 2로 특성들마다 값의 크기와 분포가 다름을 알 수 있습니다.

이러한 경우, 각 특성들의 가중치를 정함에 있어, 편향이 발생할 수 있으므로 ( ) 특성 스케일링을 해줘야 합니다.

대표적인 특성 스케일링 방법에는 표준화와 정규화가 있는데,
표준화는 평균을 0, 분산을 1로 스케일링 하는것으로 하한, 상한이 없고
정규화는 각 특성의 최소, 최대값을 지정하여 스케일링 하는것 입니다.

( ) 먼저 표준화를 해보면,
sklearn의 standardScaler를 import 하고,
수치형 데이터셋인 housing_num을 학습시켜 변환하면,
( ) 평균이 -3.63e-17 즉 0에 수렴하는 것을 알 수 있고,
표준편차가 1.00003, 즉 1에 수렴하는 것을 알 수 있으며, 각각의 최소 최대는 각 특성에 맞게 조정되어 있음을 알 수 있습니다.

() 그 다음 정규화 입니다.
sklearn의 MinMaxScaler를 import 하고,
똑같이 수치형 데이터들을 학습시켜 변환하면,
각 최소값이 0, 최대값이 1로 변환되었습니다.

두가지 방법 모두 장단점이 있지만, 표준화는 최소 최대의 범위가 달라 일부 알고리즘에서 작동하지 않을 수 있고,
정규화는 분포가 아닌 최소, 최대만을 가지고 스케일링 하여 이상치에 대해 너무 민감하다는 단점이 있습니다.

( ) 그다음 비수치형 특성인 ocean_proximity를 살펴 보면,
INLAND, NEAR OCEAN, UNDER 1H OCEAN등 임의의 문자열이 아님을 볼수 있고,
( ) value_count() 메서드를 통해 보면, 5개의 범주를 가지는 범주형 특성임을 알 수 있습니다.

이러한 범주형 데이터를 처리함에 있어, 바다와의 근접도를 나타내는 현재 데이터 특성에 따라,
범주형 데이터가 아닌 바다와의 거리를 측정하여 수치형 데이터로 바꾸는 방법도 있으나,

현재 데이터를 가지고 처리하려면 One-Hot Encoding이 적절합니다.

( ) Sklearn의 OneHotEncoder를 import 하고
범주형 데이터를 학습시켜 변환하면, Sipy의 희소행렬 형태로 출력되고, toarray() 메서드를 통해 numpy 배열로 바꿔보면,
각 데이터 마다, 5개의 범주중 해당하는 범주에 1, 나머지 범주에 0의 값을 가짐을 알 수 있습니다.

우리는 여기까지 housing 데이터에 정제, 스케일링, One-Hot-Encoding 총 3가지 전처리를 수행하였는데,
새로운 데이터가 들어와 처리할 때, 지금은 처리의 수가 적어, 별도로 수행 할 수있지만,
만약 처리해야할 변환기의 수가 늘어나면, 그 과정은 상당히 복잡해 질 것입니다.

( ) 그래서 sklearn에선, 여러 변환기들을 하나로 잇는 파이프라인을 지원하며,
순서대로, 연속적으로 진행되는 전처리 단계들을 하나로 묶어 편하게 처리할 수 있습니다.

( ) 먼저 수치형 특성 전처리를 묶어보면
sklearn의 pipline을 이용하여, 중간값으로 정제하는 imputer와 StandardScaler를 묶어 보면,

( ) 아무런 전처리를 하지 않았던 초기 데이터에서, null값 처리, 스케일링이 모두 수행됨을 볼 수 있고,

( ) 여기에 ColumnTransformer를 이용하여 범주형 데이터까지 처리하면,
( ) 수치형 데이터 특성 8개에, One-Hot-Encoding의 결과인 5개의 특성이 추가되어
16512개의 데이터셋에 대해 13개의 특성이 생김을 볼 수 있습니다.

( ) 이렇게 전처리가 끝나면 이제 처리한 데이터를 가지고 모델을 선택하여 훈련해야 하는데,
우리는 최초에 회귀를 하기로 정했으므로, 대표적인 회귀 알고리즘은 선형 회귀, 의사 결정 트리, 랜덤 포레스트 모델을 훈련시켜 보겠습니다.

모델을 훈련하기 앞서
( ) 현재 보이는 코드와 같이, 학습 데이터 모두를 가지고 학습을 수행하면 오차가 0,
즉 Overfitting이 발생할 수 있으므로,

( ) 우리는 훈련세트를 나누어 교차적으로 검증하는 교차 검증 방법으로 훈련할 것입니다.

( ) 먼저 선형회귀 입니다.
Sklearn의 LinearRegression와 cross_val_score를 import하고
전처리를 끝낸 데이터를 10개로 나누어, 교차 검증시켜 그 평균을 내면,
평균 69204달러의 오차를 내며,

같은 방법으로 
( ) 의사결정트리 DecisionTreeRegressor로 수행하면,
평균 69081달러,

( ) RandomForestRegressor로 수행하면
49432달러의 오차를 냅니다.

현재 우리는 3가지 모델을 학습시켜본 결과 선형 회귀나, 의사결정트리보다 랜덤 포레스트가 더 나은 결과를 보여주었는데.
그 수행 과정에서 우리는 임의의 하이퍼 파라미터를 가지고 수행하였습니다.

그 하이퍼 파라미터를 바꾸고 수행하면
( ) 그 결과도 바뀌는데,
우리는 최상의 모델을 찾는 것 만이 아닌, 최상의 하이퍼 파라미터를 찾아야 합니다.

Hyper Parameter를 갱신하는 대표적인 방법으로
가능한 모든 조합을 하나하나 때려넣는 그리디한 탐색방법과

가능한 조합중 지정한 횟수만큼, 임의의 조합을 평가하는 랜덤 탐색등이 있습니다.

먼저 그리디한 방법을 보면,
앞서 최상의 결과를 보였던 RandomForest에 대해 n_estimators, max_features, bootstrap 총 3가지 하이퍼 파라미터를 가지고
모든 경우의 수를 돌려보면,

각각의 경우에 대해 결과가 출력되고,
그중 최상의 하이퍼 파라미터는 max_feature가 8, n_estimator가 30일때, 최상의 결과임을 확인 할 수 있습니다

그리디 알고리즘은 모든 경우의 수를 계산하므로, 하이퍼 파라미터의 경우의수가 많아질 수록 시간이 오래걸리는데,
그럴땐 